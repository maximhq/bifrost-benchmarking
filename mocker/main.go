// Package main implements a mock OpenAI API server.
package main

import (
	"encoding/json"
	"flag"
	"fmt"
	"log"
	"net/http"
	"time"

	"github.com/maximhq/bifrost/core/schemas"
)

// OpenAIResponse mimics the structure of a successful response from the OpenAI Chat Completions API.
// It includes fields for the completion ID, object type, choices, model, creation timestamp,
// service tier, system fingerprint, and token usage statistics.
type OpenAIResponse struct {
	ID                string                          `json:"id"`                 // Unique identifier for the completion
	Object            string                          `json:"object"`             // Type of completion (e.g., chat.completion)
	Choices           []schemas.BifrostResponseChoice `json:"choices"`            // Array of completion choices generated by the model
	Model             string                          `json:"model"`              // Model used for generating the completion
	Created           int                             `json:"created"`            // Unix timestamp of when the completion was created
	ServiceTier       *string                         `json:"service_tier"`       // Optional service tier used for the request
	SystemFingerprint *string                         `json:"system_fingerprint"` // Optional system fingerprint associated with the generation
	Usage             schemas.LLMUsage                `json:"usage"`              // Token usage statistics for the completion request
}

// OpenAIError represents the error response structure from the OpenAI API.
// It includes detailed error information and event tracking.
type OpenAIError struct {
	EventID string `json:"event_id"` // Unique identifier for the error event
	Type    string `json:"type"`     // Type of error (e.g., "invalid_request_error")
	Error   struct {
		Type    string      `json:"type"`     // Specific error type (e.g., "invalid_api_key")
		Code    string      `json:"code"`     // Error code (e.g., "invalid_api_key")
		Message string      `json:"message"`  // Human-readable error message
		Param   interface{} `json:"param"`    // Optional parameter that caused the error
		EventID string      `json:"event_id"` // Event ID for tracking, often mirrors the outer EventID
	} `json:"error"` // Nested error details
}

var (
	port    int // port stores the port number the mock server will listen on.
	latency int // latency stores the simulated latency in milliseconds for responses.
)

// init parses command-line flags for port and latency configuration.
func init() {
	flag.IntVar(&port, "port", 8000, "Port for the mock server to listen on")
	flag.IntVar(&latency, "latency", 0, "Latency in milliseconds to simulate")
}

// StrPtr creates a pointer to a string value.
// This is a utility function often used when a struct field expects a *string,
// but you have a literal string.
func StrPtr(s string) *string {
	return &s
}

// mockOpenAIHandler handles incoming HTTP requests to the /v1/chat/completions endpoint.
// It simulates a delay if latency is configured, then returns a mock OpenAI chat completion response.
// It only accepts POST requests; other methods will result in a 405 Method Not Allowed error.
func mockOpenAIHandler(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		http.Error(w, "Only POST method is allowed", http.StatusMethodNotAllowed)
		return
	}

	// Simulate latency if a positive value is provided via command-line flag.
	if latency > 0 {
		time.Sleep(time.Duration(latency) * time.Millisecond)
	}

	// Create a mock response structure.
	mockChoiceMessage := schemas.BifrostResponseChoiceMessage{
		Role:    schemas.RoleAssistant,                                              // The role of the message author (assistant).
		Content: StrPtr("This is a mocked response from the OpenAI mocker server."), // The content of the message.
	}
	mockChoice := schemas.BifrostResponseChoice{
		Index:        0,                 // Index of the choice in the list of choices.
		Message:      mockChoiceMessage, // The message generated by the model.
		FinishReason: StrPtr("stop"),    // Reason why the model stopped generating tokens.
	}

	// Assemble the full mock OpenAIResponse.
	mockResp := OpenAIResponse{
		ID:      "cmpl-mock12345",                            // A fixed mock completion ID.
		Object:  "chat.completion",                           // The type of object, typically "chat.completion".
		Created: int(time.Now().Unix()),                      // Current Unix timestamp.
		Model:   "gpt-3.5-turbo-mock",                        // A mock model name.
		Choices: []schemas.BifrostResponseChoice{mockChoice}, // The list of choices (contains one mock choice).
		Usage: schemas.LLMUsage{ // Mock token usage statistics.
			PromptTokens:     10,
			CompletionTokens: 20,
			TotalTokens:      30,
		},
	}

	w.Header().Set("Content-Type", "application/json") // Set the content type to application/json.
	w.WriteHeader(http.StatusOK)                       // Set the HTTP status code to 200 OK.
	// Encode the mock response to JSON and write it to the response writer.
	if err := json.NewEncoder(w).Encode(mockResp); err != nil {
		log.Printf("Error encoding mock response: %v", err)
		// If encoding fails, send an internal server error.
		http.Error(w, "Failed to encode response", http.StatusInternalServerError)
	}
}

// main is the entry point of the mock server.
// It parses command-line flags, sets up an HTTP handler for the mock OpenAI endpoint,
// and starts the HTTP server.
func main() {
	flag.Parse() // Parse the command-line flags defined in init().

	// Register mockOpenAIHandler to handle all requests to /v1/chat/completions.
	http.HandleFunc("/v1/chat/completions", mockOpenAIHandler)

	addr := fmt.Sprintf(":%d", port) // Construct the server address string.
	// Log the server start information.
	log.Printf("Mock OpenAI server starting on port %d with latency %dms...\n", port, latency)
	// Start the HTTP server and log a fatal error if it fails to start.
	if err := http.ListenAndServe(addr, nil); err != nil {
		log.Fatalf("Failed to start server: %v", err)
	}
}
