// Package main implements a mock OpenAI API server.
package main

import (
	"encoding/json"
	"flag"
	"fmt"
	"log"
	"math/rand"
	"net/http"
	"strings"
	"time"

	"github.com/maximhq/bifrost/core/schemas"
)

// OpenAIResponse mimics the structure of a successful response from the OpenAI Chat Completions API.
// It includes fields for the completion ID, object type, choices, model, creation timestamp,
// service tier, system fingerprint, and token usage statistics.
type OpenAIResponse struct {
	ID                string                          `json:"id"`                 // Unique identifier for the completion
	Object            string                          `json:"object"`             // Type of completion (e.g., chat.completion)
	Choices           []schemas.BifrostResponseChoice `json:"choices"`            // Array of completion choices generated by the model
	Model             string                          `json:"model"`              // Model used for generating the completion
	Created           int                             `json:"created"`            // Unix timestamp of when the completion was created
	ServiceTier       *string                         `json:"service_tier"`       // Optional service tier used for the request
	SystemFingerprint *string                         `json:"system_fingerprint"` // Optional system fingerprint associated with the generation
	Usage             schemas.LLMUsage                `json:"usage"`              // Token usage statistics for the completion request
}

// OpenAIError represents the error response structure from the OpenAI API.
// It includes detailed error information and event tracking.
type OpenAIError struct {
	EventID string `json:"event_id"` // Unique identifier for the error event
	Type    string `json:"type"`     // Type of error
	Error   struct {
		Type    string      `json:"type"`     // Specific error type (e.g., "invalid_api_key")
		Code    string      `json:"code"`     // Error code (e.g., "invalid_api_key")
		Message string      `json:"message"`  // Human-readable error message
		Param   interface{} `json:"param"`    // Optional parameter that caused the error
		EventID string      `json:"event_id"` // Event ID for tracking, often mirrors the outer EventID
	} `json:"error"` // Nested error details
}

var (
	port       int
	latency    int
	jitter     int
	bigPayload bool
)

func init() {
	flag.IntVar(&port, "port", 8000, "Port for the mock server to listen on")
	flag.IntVar(&latency, "latency", 0, "Latency in milliseconds to simulate")
	flag.IntVar(&jitter, "jitter", 0, "Maximum jitter in milliseconds to add to latency (±jitter)")
	flag.BoolVar(&bigPayload, "big-payload", false, "Use big payload")
}

// StrPtr creates a pointer to a string value.
func StrPtr(s string) *string {
	return &s
}

// mockOpenAIHandler handles incoming HTTP requests to the /v1/chat/completions endpoint.
// It simulates a delay if latency is configured, then returns a mock OpenAI chat completion response.
// It only accepts POST requests; other methods will result in a 405 Method Not Allowed error.
func mockOpenAIHandler(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		http.Error(w, "Only POST method is allowed", http.StatusMethodNotAllowed)
		return
	}

	// Simulate latency with optional jitter
	if latency > 0 || jitter > 0 {
		actualLatency := latency
		if jitter > 0 {
			// Add random jitter: ±jitter milliseconds
			jitterOffset := rand.Intn(2*jitter+1) - jitter
			actualLatency += jitterOffset
			// Ensure latency doesn't go negative
			if actualLatency < 0 {
				actualLatency = 0
			}
		}
		if actualLatency > 0 {
			time.Sleep(time.Duration(actualLatency) * time.Millisecond)
		}
	}

	mockContent := "This is a mocked response from the OpenAI mocker server."
	if bigPayload {
		// Repeat content to generate approximately 10KB response
		// Each repetition is ~55 chars, so ~182 repetitions ≈ 10KB
		mockContent = strings.Repeat(mockContent, 182)
	}

	// Create a mock response
	mockChoiceMessage := schemas.BifrostResponseChoiceMessage{
		Role:    schemas.ModelChatMessageRole("assistant"),
		Content: StrPtr(mockContent),
	}
	mockChoice := schemas.BifrostResponseChoice{
		Index:        0,
		Message:      mockChoiceMessage,
		FinishReason: StrPtr("stop"),
	}

	randomInputTokens := rand.Intn(1000)
	randomOutputTokens := rand.Intn(1000)

	mockResp := OpenAIResponse{
		ID:      "cmpl-mock12345",
		Object:  "chat.completion",
		Created: int(time.Now().Unix()),
		Model:   "gpt-4o-mini",
		Choices: []schemas.BifrostResponseChoice{mockChoice},
		Usage: schemas.LLMUsage{
			PromptTokens:     randomInputTokens,
			CompletionTokens: randomOutputTokens,
			TotalTokens:      randomInputTokens + randomOutputTokens,
		},
	}

	w.Header().Set("Content-Type", "application/json") // Set the content type to application/json.
	w.WriteHeader(http.StatusOK)                       // Set the HTTP status code to 200 OK.
	// Encode the mock response to JSON and write it to the response writer.
	if err := json.NewEncoder(w).Encode(mockResp); err != nil {
		log.Printf("Error encoding mock response: %v", err)
		// If encoding fails, send an internal server error.
		http.Error(w, "Failed to encode response", http.StatusInternalServerError)
	}
}

// main is the entry point of the mock server.
// It parses command-line flags, sets up an HTTP handler for the mock OpenAI endpoint,
// and starts the HTTP server.
func main() {
	flag.Parse() // Parse the command-line flags defined in init().

	// Register mockOpenAIHandler to handle all requests to /v1/chat/completions.
	http.HandleFunc("/v1/chat/completions", mockOpenAIHandler)

	addr := fmt.Sprintf(":%d", port)
	if jitter > 0 {
		log.Printf("Mock OpenAI server starting on port %d with latency %dms ±%dms jitter...\n", port, latency, jitter)
	} else {
		log.Printf("Mock OpenAI server starting on port %d with latency %dms...\n", port, latency)
	}
	if err := http.ListenAndServe(addr, nil); err != nil {
		log.Fatalf("Failed to start server: %v", err)
	}
}
